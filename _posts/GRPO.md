---
title: 'GRPO: Group Relative Policy Optimization Explained'
date: 2025-04-22
permalink: /posts/2025/04/GRPO/
---

# GRPO : PPO :: Study Group : Classroom

**Group Relative Policy Optimization (GRPO)** is a new reinforcement learning (RL) algorithm introduced by DeepSeek. It offers a unique twist: rather than depending on a *critic* to guide learning‚Äîas in Proximal Policy Optimization (PPO),GRPO compares **groups** of responses to evaluate performance. This blog post breaks down how it works, why it matters, and how it changes the RL game.

---

## üîç What is GRPO?

Traditional RL algorithms like PPO require a *critic network* to estimate the value function and guide learning. GRPO skips the critic entirely. Instead, it:

- Samples a **group** of responses for a given input (or "query").
- Scores responses **relatively** within that group.
- Updates the policy based on comparative performance.

This means GRPO doesn't rely on absolute metrics. Instead, it evaluates which responses are better within the same group‚Äîlike grading on a curve.

---

## ‚úÖ Why GRPO? Key Advantages

GRPO offers several benefits over critic-based approaches:

1. **No Critic Dependency**  
   Removes the need to train or fine-tune a separate value estimator.

2. **Lower Compute Cost**  
   Eliminating the critic reduces training overhead significantly.

3. **Group-based Scalability**  
   The algorithm operates on response groups, which scales more easily for batch processing or distributed training.

---

## üß† Core Concepts (With Light Math)

Let‚Äôs walk through the core mechanics and math behind GRPO.

### 1. Group Sampling

For a query `q`, you sample a group of `G` outputs from the **old policy**:

{O_i}{i=1}^G ~ œÄ{Œ∏_old}(O | q)



This group becomes the basis for comparison.

### 2. Compute Probabilities

For each response `O_i`, calculate its probability under the **new** and **old** policies:

ratio_i = œÄ_Œ∏(O_i | q) / œÄ_{Œ∏_old}(O_i | q)


This ratio tells you whether the new policy is more or less likely to generate that response.

### 3. Advantage Estimation

Instead of using a learned value function, GRPO calculates a **relative advantage** `A_i` by comparing each response‚Äôs reward against others in the group. Think of this as a normalized "how good" score.

### 4. Clipped Objective

To avoid large updates (a common issue in PPO), GRPO clips the ratio:

L(Œ∏) = min( ratio_i * A_i, clip(ratio_i, 1 - Œµ, 1 + Œµ) * A_i )


This stabilizes learning.

### 5. KL Penalty (Regularization)

To ensure the new policy doesn't diverge too much from the old, a KL divergence term is added:

L_total = L(Œ∏) - Œª * KL[œÄ_Œ∏ || œÄ_{Œ∏_old}]


---

## üë©‚Äçüè´ A Classroom Analogy

Imagine teaching a classroom. Instead of simply marking answers right or wrong:

- You collect all student answers.
- Compare them against each other.
- Identify which ones are best and why.
- Reward the top ones.
- Help the others improve based on peer comparison.

This is GRPO‚Äôs core idea: **learning from relative performance**, not absolute correctness.

---

## üîÅ GRPO Training Loop (Simplified)

Here's what the GRPO algorithm looks like in practice:

1. **Sample query** `q`.
2. **Generate a group of outputs** `{O‚ÇÅ, O‚ÇÇ, ..., O_G}` from the old policy.
3. **Score responses** with a reward model or heuristics (e.g., helpfulness, relevance).
4. **Normalize rewards** and compute relative advantages `{A‚ÇÅ, A‚ÇÇ, ..., A_G}`.
5. **Calculate update ratios** using the new and old policy.
6. **Apply clipped updates**.
7. **Subtract KL penalty** for regularization.
8. **Repeat** across all training data.

---

## ‚ú® Summary

| Component        | GRPO Description |
|------------------|------------------|
| **Critic-Free** | No value function or critic needed |
| **Group Comparison** | Rewards are relative, not absolute |
| **Clipping** | Stabilizes learning |
| **KL Penalty** | Prevents policy drift |
| **Scalable** | Efficient for batch or distributed training |

GRPO is an elegant evolution in policy optimization. By leaning into **group-wise comparison**, it simplifies the RL pipeline and reduces overhead‚Äîwhile maintaining strong performance.

---

Want to dive deeper into GRPO or implement it from scratch? Stay tuned for a follow-up post with code, eq